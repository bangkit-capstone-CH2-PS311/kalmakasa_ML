# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17LLXlhFcg29lLOTeL02H3h8izTkB9AHP
"""

from google.colab import drive
drive.mount('/content/drive')

import json
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.text import tokenizer_from_json
from tensorflow.keras.preprocessing.sequence import pad_sequences
import re
from sklearn.preprocessing import LabelEncoder
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer        # module for stemming
import io
nltk.download('stopwords')
nltk.download('punkt')

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (logs.get('accuracy') > 0.99):
            print("\n Target telah tercapai")
            self.model.stop_training = True

with open('/content/drive/MyDrive/ML Capstone Bangkit 2023H2/Data/intents_indo_3.json', 'r') as f:
    data = json.load(f)

df = pd.DataFrame(data['intents'])
df

responses = {}
for intent in data['intents']:
  responses[intent['tag']] = intent['responses']

tag_to_search = "greeting"
responses_for_tag = df.loc[df['tag'] == tag_to_search, 'responses'].values
responses_for_tag

df_train = df.explode('patterns').reset_index(drop=True)
df_train.drop('responses', axis=1, inplace=True)
df_train

df_train['patterns'] = df_train['patterns'].str.lower()
df_train['patterns'] = df_train['patterns'].str.replace(r'/r/|[^\w\s]|https?://\S+|www\.\S+|\#\w+', '', regex=True)
df_train

vocab_size = 2000
embedding_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = "<OOV>"

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(df_train['patterns'])
sequences = tokenizer.texts_to_sequences(df_train['patterns'])

x_train = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)

le = LabelEncoder()
y_train = le.fit_transform(df_train['tag'])

tokenizer_json = tokenizer.to_json()
with io.open('tokenizer.json', 'w', encoding='utf-8') as f:
    f.write(json.dumps(tokenizer_json, ensure_ascii=False))

input_shape = x_train.shape[1]
vocabulary = len(tokenizer.word_index)
output_shape = le.classes_.shape[0]

i = tf.keras.layers.Input(shape=(input_shape,))
x = tf.keras.layers.Embedding(vocabulary+1, 16)(i)
x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))(x)
# x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16))(x)
# x = tf.keras.layers.GlobalAveragePooling1D()(x)
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(32)(x)
x = tf.keras.layers.Dense(output_shape, activation="softmax")(x)
model = tf.keras.models.Model(i, x)

model.compile(loss="sparse_categorical_crossentropy", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), metrics=["accuracy"])

callback = myCallback()

history = model.fit(x_train, y_train, epochs=500, callbacks=callback)

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

model.save("model_chatbot.h5")

